{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Reinforcement Learning Hands-On: Chapter 04.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0ooHCtyl9C78IZGq7JFxC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aboelela924/Deep-Reinforcement-Learning-Hands-On/blob/master/Deep_Reinforcement_Learning_Hands_On_Chapter_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzHPH1G-pC8t",
        "outputId": "d11b0f54-6e21-4643-feb5-76e033bfc29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKqIOZyVpoC5"
      },
      "source": [
        "import gym \n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfMD0eNuosDR"
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNxegqLCpYeB"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obervtion_size, hidden_size, number_of_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obervtion_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, number_of_actions)\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7KSrZNQtOTn"
      },
      "source": [
        "Episode = namedtuple(\"Episode\", field_names=[\"reward\", \"steps\"])\n",
        "EpisodeStep = namedtuple(\"EpisodeStep\", field_names=[\"observation\", \"action\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MGgnTP4trsP"
      },
      "source": [
        "def iterate_batches(env, net, batch_size):\n",
        "    batch = []\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = []\n",
        "    obs = env.reset()\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    while True:\n",
        "        obs_tensor = torch.FloatTensor([obs])\n",
        "        action_probabilites = net(obs_tensor)\n",
        "        action_probabilites = action_probabilites.data.numpy()[0]\n",
        "        action = np.random.choice(len(action_probabilites), p=action_probabilites)\n",
        "        new_state, reward, is_done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        episode_steps.append(EpisodeStep(observation=new_state, action=action))\n",
        "        if is_done:\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "            episode_reward = 0.0\n",
        "            episode_steps = []\n",
        "            obs = env.reset()\n",
        "            if(len(batch) == batch_size):\n",
        "                yield batch\n",
        "                batch = []\n",
        "        obs = new_state"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFTvF3WRxFtf"
      },
      "source": [
        "def filter(batch, percentile):\n",
        "    rewards = list(map(lambda e: e.reward, batch))\n",
        "    rewards_bound = np.percentile(rewards, percentile)\n",
        "    reward_mean = float(np.mean(rewards))\n",
        "\n",
        "    train_observations = []\n",
        "    train_actions = []\n",
        "\n",
        "    for episode in batch:\n",
        "        if episode.reward < rewards_bound:\n",
        "            continue\n",
        "        train_observations.extend(map(lambda episode_step: episode_step.observation , episode.steps))\n",
        "        train_actions.extend(map(lambda episode_step: episode_step.action , episode.steps))\n",
        "    \n",
        "    return torch.FloatTensor(train_observations), torch.FloatTensor(train_actions), rewards_bound, reward_mean"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eQqxcEK0vFa"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "observation_size = env.observation_space.shape[0]\n",
        "actions_size = env.action_space.n\n",
        "\n",
        "net = Net(observation_size, HIDDEN_SIZE, actions_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "for i, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "    train_observations, train_actions, rewards_bound, reward_mean = filter(batch, PERCENTILE)\n",
        "    predicted_actions = net(train_observations)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(train_actions, predicted_actions)\n",
        "    loss.backwards()\n",
        "    optimizer.step()\n",
        "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (i, loss.item(), reward_mean, rewards_bound))\n",
        "    if reward_mean > 199:\n",
        "        print(\"Solved!\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}